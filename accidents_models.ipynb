{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import h2o\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "from shapely.geometry import Polygon\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from statsmodels.tools.tools import add_constant\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from h2o.automl import H2OAutoML\n",
    "# from tpot import TPOTClassifier\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV,ParameterGrid\n",
    "import xgboost\n",
    "from sklearn.metrics import mean_squared_error, r2_score, roc_auc_score\n",
    "from sklearn.linear_model import LinearRegression \n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.svm import SVC\n",
    "import sys\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn import metrics\n",
    "import os\n",
    "import mlflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///C:/Users/PREDATOR/Music/ML_project/mlflowruns/273055796817312252', creation_time=1701179561364, experiment_id='273055796817312252', last_update_time=1701179561364, lifecycle_stage='active', name='ML-Project-Group-6', tags={}>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "os.environ[\"LOGNAME\"] = \"ML_Project\"\n",
    "# mlflow.create_experiment(name=\"ML-Project-Group-6\")  #create if you haven't create\n",
    "mlflow.set_experiment(experiment_name=\"ML-Project-Group-6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_sub = pd.read_csv(\"sub_accidents.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_sub = accidents_sub[accidents_sub[\"Start_Year\"]>2020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3047692, 38)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accidents_sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = accidents_sub.drop(columns=['ID', 'Severity', 'Delay(min)', 'Street', 'City', 'County', 'State', 'Zipcode',\n",
    "       'Start_Year', 'Start_Month', 'Start_Day', 'Start_Lat', 'Start_Lng',\n",
    "       'Start_time','Distance(km)', 'Delay_ln', 'Severity_new'])\n",
    "Y = accidents_sub.loc[:, ['Severity', 'Severity_new', 'Delay(min)', 'Delay_ln', 'Distance(km)']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3047692, 21),\n",
       " (3047692, 5),\n",
       " (2742922, 21),\n",
       " (304770, 21),\n",
       " (2742922,),\n",
       " (304770,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y[\"Severity_new\"], test_size = 0.10, random_state=370, stratify=Y[\"Severity_new\"])\n",
    "\n",
    "assert len(X_train)  == len(Y_train)\n",
    "assert len(X_test)   == len(Y_test)\n",
    "X.shape, Y.shape,X_train.shape, X_test.shape, Y_train.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# fixing imbalance by under sampling (Tomek links) (it didn\\'t help)\\ntl = TomekLinks(n_jobs=-1, sampling_strategy=\"Majority\")\\n\\n# fit predictor and target variable\\nX_train_tl, Y_train_tl = tl.fit_resample(X_train, Y_train[\"Severity_new\"])'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# fixing imbalance by under sampling (Tomek links) (it didn't help)\n",
    "tl = TomekLinks(n_jobs=-1, sampling_strategy=\"Majority\")\n",
    "\n",
    "# fit predictor and target variable\n",
    "X_train_tl, Y_train_tl = tl.fit_resample(X_train, Y_train[\"Severity_new\"])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3047692, 21), (3047692, 5), (490126, 21), (490126,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fixing imbalance by under sampling (random under sampling)\n",
    "rus = RandomUnderSampler(random_state=370, replacement=False)\n",
    "\n",
    "# fit predictor and target varialbe\n",
    "X_train_rus, Y_train_rus = rus.fit_resample(X_train, Y_train)\n",
    "\n",
    "X.shape, Y.shape, X_train_rus.shape, Y_train_rus.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried to fit a regression model with Delay and distance as labels but no appropriate model have found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model = SGDRegressor(max_iter=1000, tol=1e-3, random_state=370)\\n\\nmodel.fit(X_train, Y_train[\"Distance(km)\"])\\n\\n# Make predictions on the validation set\\ny_train_pred = model.predict(X_train)\\n\\n# Calculate metrics\\nmse = mean_squared_error(Y_train[\"Distance(km)\"], y_train_pred)\\nr2 = r2_score(Y_train[\"Distance(km)\"], y_train_pred)\\nprint(f\\'Mean Squared Error on Test Set: {mse}\\')\\nprint(f\\'R_squared on Test Set: {r2}\\')'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''model = SGDRegressor(max_iter=1000, tol=1e-3, random_state=370)\n",
    "\n",
    "model.fit(X_train, Y_train[\"Distance(km)\"])\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_train_pred = model.predict(X_train)\n",
    "\n",
    "# Calculate metrics\n",
    "mse = mean_squared_error(Y_train[\"Distance(km)\"], y_train_pred)\n",
    "r2 = r2_score(Y_train[\"Distance(km)\"], y_train_pred)\n",
    "print(f'Mean Squared Error on Test Set: {mse}')\n",
    "print(f'R_squared on Test Set: {r2}')'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2497859\n",
       "1     245063\n",
       "Name: Severity_new, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PREDATOR\\anaconda3\\lib\\site-packages\\mlflow\\data\\dataset_source_registry.py:143: UserWarning: The specified dataset source can be interpreted in multiple ways: LocalArtifactDatasetSource, LocalArtifactDatasetSource. MLflow will assume that this is a LocalArtifactDatasetSource source.\n",
      "  return _dataset_source_registry.resolve(\n",
      "c:\\Users\\PREDATOR\\anaconda3\\lib\\site-packages\\mlflow\\data\\pandas_dataset.py:134: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  return _infer_schema(self._df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  LogisticRegression(n_jobs=-1, random_state=370) Scores:  [0.82 0.82 0.82 0.82 0.82] - Scores mean:  0.8173979713900781 - Scores std (lower better): 0.0014134375994906685\n",
      "[234457  43084   5753  21476]\n",
      "ROCAUC score: 0.8167415944868098\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.84      0.91    277541\n",
      "           1       0.33      0.79      0.47     27229\n",
      "\n",
      "    accuracy                           0.84    304770\n",
      "   macro avg       0.65      0.82      0.69    304770\n",
      "weighted avg       0.92      0.84      0.87    304770\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.84      0.91    277541\n",
      "           1       0.33      0.79      0.47     27229\n",
      "\n",
      "    accuracy                           0.84    304770\n",
      "   macro avg       0.65      0.82      0.69    304770\n",
      "weighted avg       0.92      0.84      0.87    304770\n",
      "\n",
      "Model saved in run 820d1f0eec564aa39b7b9875d2a25628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PREDATOR\\anaconda3\\lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  RandomForestClassifier(n_jobs=-1, oob_score=True, random_state=370) Scores:  [0.84 0.84 0.84 0.84 0.84] - Scores mean:  0.840742989697475 - Scores std (lower better): 0.0018569554764470812\n",
      "[239351  38190   4804  22425]\n",
      "ROCAUC score: 0.8429845833134345\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.86      0.92    277541\n",
      "           1       0.37      0.82      0.51     27229\n",
      "\n",
      "    accuracy                           0.86    304770\n",
      "   macro avg       0.68      0.84      0.71    304770\n",
      "weighted avg       0.93      0.86      0.88    304770\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.86      0.92    277541\n",
      "           1       0.37      0.82      0.51     27229\n",
      "\n",
      "    accuracy                           0.86    304770\n",
      "   macro avg       0.68      0.84      0.71    304770\n",
      "weighted avg       0.93      0.86      0.88    304770\n",
      "\n",
      "Model saved in run ad890249007f4312a99b20f8a0aa713a\n",
      "Model:  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=1, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=200, n_jobs=-1,\n",
      "              num_parallel_tree=None, random_state=None, ...) Scores:  [0.82 0.82 0.82 0.82 0.82] - Scores mean:  0.8195831230135514 - Scores std (lower better): 0.0014036565274645873\n",
      "[236069  41472   5792  21437]\n",
      "ROCAUC score: 0.81892952169639\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.85      0.91    277541\n",
      "           1       0.34      0.79      0.48     27229\n",
      "\n",
      "    accuracy                           0.84    304770\n",
      "   macro avg       0.66      0.82      0.69    304770\n",
      "weighted avg       0.92      0.84      0.87    304770\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.85      0.91    277541\n",
      "           1       0.34      0.79      0.48     27229\n",
      "\n",
      "    accuracy                           0.84    304770\n",
      "   macro avg       0.66      0.82      0.69    304770\n",
      "weighted avg       0.92      0.84      0.87    304770\n",
      "\n",
      "Model saved in run 6b0f46ef102e4abcb77b3dd41f390425\n",
      "Model:  AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
      "                   learning_rate=0.001, n_estimators=200, random_state=370) Scores:  [0.82 0.82 0.82 0.82 0.82] - Scores mean:  0.8174265355318788 - Scores std (lower better): 0.0014036299531711453\n",
      "[234484  43057   5754  21475]\n",
      "ROCAUC score: 0.8167718731738094\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.84      0.91    277541\n",
      "           1       0.33      0.79      0.47     27229\n",
      "\n",
      "    accuracy                           0.84    304770\n",
      "   macro avg       0.65      0.82      0.69    304770\n",
      "weighted avg       0.92      0.84      0.87    304770\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.84      0.91    277541\n",
      "           1       0.33      0.79      0.47     27229\n",
      "\n",
      "    accuracy                           0.84    304770\n",
      "   macro avg       0.65      0.82      0.69    304770\n",
      "weighted avg       0.92      0.84      0.87    304770\n",
      "\n",
      "Model saved in run d137a9a00f154fbc90c16a2059564de9\n"
     ]
    }
   ],
   "source": [
    "from mlflow.data.pandas_dataset import PandasDataset\n",
    "\n",
    "\n",
    "lr = LogisticRegression(random_state=370, n_jobs=-1)\n",
    "rf = RandomForestClassifier(random_state=370, n_jobs=-1, oob_score=True)\n",
    "sgb = xgboost.XGBClassifier(max_depth=1, n_estimators=200, n_jobs=-1)\n",
    "ada = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=200, learning_rate=0.001, random_state=370)\n",
    "\n",
    "# models = [ lr, rf, sgb, ada]\n",
    "models = {\"Logistic Regression\": lr, \"Random Forest\":rf, \"XGBoost\":sgb, \"Adaboost\":ada}\n",
    "\n",
    "dataset: PandasDataset = mlflow.data.from_pandas(X_train_rus, source='')\n",
    "\n",
    "\n",
    "#perform cross validation using KFold\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "kfold = KFold(n_splits = 5, shuffle = True, random_state=370)\n",
    "for name,model in models.items():\n",
    "    with mlflow.start_run(run_name=f\"Model {name}\", nested=True):\n",
    "        mlflow.log_input(dataset, context=\"training\")\n",
    "        score = cross_val_score(model, X_train_rus, Y_train_rus, cv=kfold, scoring='accuracy', n_jobs=-1)  #f1, recall, precision, accuracy\n",
    "        np.set_printoptions(precision = 2)\n",
    "        print(\"Model: \", model, \"Scores: \", score, \"- Scores mean: \", score.mean(), \"- Scores std (lower better):\", score.std()) \n",
    "        params = {\"Model\":model, \"Scores\":score, \"Scores mean\":score.mean(), \"Scores std-lower better \":score.std()}\n",
    "        mlflow.log_params(params=params)\n",
    "        fit = model.fit(X_train_rus, Y_train_rus)\n",
    "        yhat = fit.predict(X_test)\n",
    "        conf_matrix = confusion_matrix(Y_test, yhat)\n",
    "        true_positive = conf_matrix[0][0]\n",
    "        true_negative = conf_matrix[1][1]\n",
    "        false_positive = conf_matrix[0][1]\n",
    "        false_negative = conf_matrix[1][0]\n",
    "        mlflow.log_metric(\"true_positive\", true_positive)\n",
    "        mlflow.log_metric(\"true_negative\", true_negative)\n",
    "        mlflow.log_metric(\"false_positive\", false_positive)\n",
    "        mlflow.log_metric(\"false_negative\", false_negative)\n",
    "        print(confusion_matrix(Y_test, yhat).ravel())\n",
    "        mlflow.log_metric(\"ROCAUC_score\", roc_auc_score(Y_test, yhat))\n",
    "        print('ROCAUC score:',roc_auc_score(Y_test, yhat))      \n",
    "        print(classification_report(Y_test, yhat))\n",
    "        cr = classification_report(Y_test, yhat, output_dict=True) \n",
    "        mlflow.log_metric(\"accuracy\", cr.pop(\"accuracy\"))\n",
    "        for class_or_avg, metrics_dict in cr.items():\n",
    "            for metric, value in metrics_dict.items():\n",
    "                mlflow.log_metric(class_or_avg + '_' + metric,value)     \n",
    "        print(classification_report(Y_test, yhat))\n",
    "        print(\"Model saved in run %s\" % mlflow.active_run().info.run_uuid)\n",
    "        mlflow.sklearn.log_model(fit, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=f\"Grid-CV-RF\", nested=True):\n",
    "    mlflow.log_input(dataset, context=\"training\")\n",
    "    # mlflow.sklearn.autolog()\n",
    "    param_grid = {\"n_estimators\": [100, 200, 300], \"criterion\": [\"gini\", \"entropy\"], \"max_depth\": np.arange(1, 10)}\n",
    "    mlflow.log_params(params=param_grid)\n",
    "    model = RandomForestClassifier(n_jobs=-1, random_state=370, oob_score=True)\n",
    "    grid = GridSearchCV(model, param_grid, refit=True, n_jobs=-1)\n",
    "    grid.fit(X_train_rus, Y_train_rus)\n",
    "    print(grid.best_params_)\n",
    "    yhat = grid.predict(X_test)\n",
    "    grid_best_model = grid.best_estimator_\n",
    "    print(grid_best_model)\n",
    "    mlflow.log_params(params= grid.best_params_)\n",
    "    mlflow.log_metric('best score', grid.best_score_)\n",
    "    # mlflow.sklearn.log_model(grid_best_model, \"gridcv_classifier_model\")\n",
    "    print(classification_report(Y_test, yhat))\n",
    "    # print(\"Model saved in run %s\" % mlflow.active_run().info.run_uuid)\n",
    "    print(classification_report(Y_test, yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negative =  243805  False Negative =  6193 True Positive =  21036 False Positive =  6193\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.88      0.92    277541\n",
      "           1       0.38      0.77      0.51     27229\n",
      "\n",
      "    accuracy                           0.87    304770\n",
      "   macro avg       0.68      0.83      0.72    304770\n",
      "weighted avg       0.92      0.87      0.89    304770\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name=f\"Random-Forest-Normal-RS\", nested=True):\n",
    "    mlflow.log_input(dataset, context=\"training\")\n",
    "    model = RandomForestClassifier(n_estimators= 200, criterion=\"gini\", max_depth= 9, n_jobs=-1, random_state=370)\n",
    "    params = {\"n_estimators\":200, \"criterion\":\"gini\", \"max_depth\":9, \"n_jobs\":-1, \"random_state\":370}\n",
    "    mlflow.log_params(params=params)\n",
    "    model.fit(X_train_rus, Y_train_rus)\n",
    "    yhat = model.predict(X_test)\n",
    "    tn, fp, fn, tp = confusion_matrix(Y_test, yhat).ravel()\n",
    "    mlflow.log_metric(\"true_positive\", tp)\n",
    "    mlflow.log_metric(\"true_negative\", tn)\n",
    "    mlflow.log_metric(\"false_positive\", fp)\n",
    "    mlflow.log_metric(\"false_negative\", fn)\n",
    "    print(\"True Negative = \",tn, \" False Negative = \",fn, \"True Positive = \", tp, \"False Positive = \", fn)\n",
    "    print(classification_report(Y_test, yhat))\n",
    "    cr = classification_report(Y_test, yhat, output_dict=True)\n",
    "    mlflow.log_metric(\"accuracy\", cr.pop(\"accuracy\"))\n",
    "    for class_or_avg, metrics_dict in cr.items():\n",
    "        for metric, value in metrics_dict.items():\n",
    "            mlflow.log_metric(class_or_avg + '_' + metric,value)\n",
    "    mlflow.sklearn.log_model(model, \"rf_normal\")\n",
    "    mlflow.sklearn.save_model(model, \"rf_normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PREDATOR\\anaconda3\\lib\\site-packages\\mlflow\\data\\dataset_source_registry.py:143: UserWarning: The specified dataset source can be interpreted in multiple ways: LocalArtifactDatasetSource, LocalArtifactDatasetSource. MLflow will assume that this is a LocalArtifactDatasetSource source.\n",
      "  return _dataset_source_registry.resolve(\n",
      "c:\\Users\\PREDATOR\\anaconda3\\lib\\site-packages\\mlflow\\data\\pandas_dataset.py:134: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  return _infer_schema(self._df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negative =  243897  False Negative =  6206 True Positive =  21023 False Positive =  6206\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.88      0.92    277541\n",
      "           1       0.38      0.77      0.51     27229\n",
      "\n",
      "    accuracy                           0.87    304770\n",
      "   macro avg       0.68      0.83      0.72    304770\n",
      "weighted avg       0.92      0.87      0.89    304770\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset: PandasDataset = mlflow.data.from_pandas(X_train, source='')\n",
    "with mlflow.start_run(run_name=f\"Random-Forest-Balanced-Subsample\", nested=True):\n",
    "    mlflow.log_input(dataset, context=\"training\")\n",
    "    model = RandomForestClassifier(n_estimators= 200, criterion=\"gini\", max_depth= 9, n_jobs=-1, random_state=370, class_weight=\"balanced_subsample\")\n",
    "    params = {\"n_estimators\":200, \"criterion\":\"gini\", \"max_depth\":9, \"n_jobs\":-1, \"random_state\":370, \"class_weight\":\"balanced_subsample\"}\n",
    "    mlflow.log_params(params=params)\n",
    "    model.fit(X_train, Y_train)\n",
    "    yhat = model.predict(X_test)\n",
    "    tn, fp, fn, tp = confusion_matrix(Y_test, yhat).ravel()\n",
    "    mlflow.log_metric(\"true_positive\", tp)\n",
    "    mlflow.log_metric(\"true_negative\", tn)\n",
    "    mlflow.log_metric(\"false_positive\", fp)\n",
    "    mlflow.log_metric(\"false_negative\", fn)\n",
    "    print(\"True Negative = \",tn, \" False Negative = \",fn, \"True Positive = \", tp, \"False Positive = \", fn)\n",
    "    print(classification_report(Y_test, yhat))\n",
    "    cr = classification_report(Y_test, yhat, output_dict=True)\n",
    "    mlflow.log_metric(\"accuracy\", cr.pop(\"accuracy\"))\n",
    "    for class_or_avg, metrics_dict in cr.items():\n",
    "        for metric, value in metrics_dict.items():\n",
    "            mlflow.log_metric(class_or_avg + '_' + metric,value)\n",
    "    mlflow.sklearn.log_model(model, \"rf_balanced_sub\")\n",
    "    mlflow.sklearn.save_model(model, \"rf_balanced_sub\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negative =  243634  False Negative =  6182 True Positive =  21047 False Positive =  6182\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.88      0.92    277541\n",
      "           1       0.38      0.77      0.51     27229\n",
      "\n",
      "    accuracy                           0.87    304770\n",
      "   macro avg       0.68      0.83      0.72    304770\n",
      "weighted avg       0.92      0.87      0.89    304770\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name=f\"Random-Forest-Balanced\", nested=True):\n",
    "    mlflow.log_input(dataset, context=\"training\")\n",
    "    model = RandomForestClassifier(n_estimators= 200, criterion=\"gini\", max_depth= 9, n_jobs=-1, random_state=370, class_weight=\"balanced\")\n",
    "    params = {\"n_estimators\":200, \"criterion\":\"gini\", \"max_depth\":9, \"n_jobs\":-1, \"random_state\":370, \"class_weight\":\"balanced\"}\n",
    "    mlflow.log_params(params=params)\n",
    "    model.fit(X_train, Y_train)\n",
    "    yhat = model.predict(X_test)\n",
    "    tn, fp, fn, tp = confusion_matrix(Y_test, yhat).ravel()\n",
    "    mlflow.log_metric(\"true_positive\", tp)\n",
    "    mlflow.log_metric(\"true_negative\", tn)\n",
    "    mlflow.log_metric(\"false_positive\", fp)\n",
    "    mlflow.log_metric(\"false_negative\", fn)\n",
    "    print(\"True Negative = \",tn, \" False Negative = \",fn, \"True Positive = \", tp, \"False Positive = \", fn)\n",
    "    print(classification_report(Y_test, yhat))\n",
    "    cr = classification_report(Y_test, yhat, output_dict=True)\n",
    "    mlflow.log_metric(\"accuracy\", cr.pop(\"accuracy\"))\n",
    "    for class_or_avg, metrics_dict in cr.items():\n",
    "        for metric, value in metrics_dict.items():\n",
    "            mlflow.log_metric(class_or_avg + '_' + metric,value)\n",
    "    mlflow.sklearn.log_model(model, \"rf_balanced\")\n",
    "    mlflow.sklearn.save_model(model, \"rf_balanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Range of clusters to test\n",
    "n_clusters_range = range(1, 11)\n",
    "bic_scores = []\n",
    "\n",
    "# Fit the model for each number of clusters and compute BIC\n",
    "with mlflow.start_run(run_name=f\"GMM-Clustering\", nested=True):\n",
    "    mlflow.log_params(params={\"n_cluster\":n_clusters_range})\n",
    "    for n_clusters in n_clusters_range:\n",
    "        gmm = GaussianMixture(n_components=n_clusters, random_state=42)\n",
    "        gmm.fit(X_train)\n",
    "    \n",
    "        # BIC (Bayesian Information Criterion) score\n",
    "        bic_scores.append(gmm.bic(X_train))\n",
    "\n",
    "    # Plot the elbow plot\n",
    "    # plt.figure(figsize=(10, 6))\n",
    "    # plt.plot(n_clusters_range, bic_scores, marker='o')\n",
    "    # plt.title('Elbow Plot for Gaussian Mixture Model')\n",
    "    # plt.xlabel('Number of Clusters')\n",
    "    # plt.ylabel('BIC Score')\n",
    "    # plt.grid(True)\n",
    "    # plt.show()\n",
    "\n",
    "    # Choose the optimal number of clusters based on the elbow plot\n",
    "    optimal_n_clusters = np.argmin(bic_scores) + 1  # +1 because Python is zero-indexed\n",
    "    print(f'Optimal Number of Clusters: {optimal_n_clusters}')\n",
    "    mlflow.log_metrics(\"optimal_no_of_cluster\", optimal_n_clusters)\n",
    "\n",
    "    # Train the model with the optimal number of clusters\n",
    "    optimal_gmm = GaussianMixture(n_components=optimal_n_clusters, random_state=42)\n",
    "    optimal_gmm.fit(X_train)\n",
    "\n",
    "    # Predict the clusters for the testing data\n",
    "    test_labels = optimal_gmm.predict(X_test)\n",
    "\n",
    "    # Evaluate the model on the testing data\n",
    "    test_score = optimal_gmm.score(X_test)\n",
    "    silhouette_score = metrics.silhouette_score(X_test, test_labels)\n",
    "\n",
    "    print(f'Log-Likelihood on Test Data: {test_score}')\n",
    "    print(f'Silhouette Score on Test Data: {silhouette_score}')\n",
    "    mlflow.log_metrics('test_scores', test_score)\n",
    "    mlflow.log_metrics('silhouette_score', silhouette_score)\n",
    "    mlflow.sklearn.log_model(optimal_gmm, \"gaussian_mixture\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
