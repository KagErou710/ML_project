{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import h2o\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "from shapely.geometry import Polygon\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from statsmodels.tools.tools import add_constant\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from h2o.automl import H2OAutoML\n",
    "from tpot import TPOTClassifier\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost\n",
    "from sklearn.metrics import mean_squared_error, r2_score, roc_auc_score\n",
    "from sklearn.linear_model import LinearRegression \n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.svm import SVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_sub = pd.read_csv(\"C:\\\\Users\\\\reza3\\\\OneDrive\\\\Desktop\\\\AIT\\\\Machine learning\\\\group project\\\\data\\\\sub_accidents.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_sub.drop(columns=\"Unnamed: 0\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_sub.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = accidents_sub.drop(columns=['ID', 'Severity', 'Delay(min)', 'Street', 'City', 'County', 'State', 'Zipcode',\n",
    "       'Start_Year', 'Start_Month', 'Start_Day',\n",
    "       'Start_time','Distance(km)', 'Delay_ln', 'Severity_new'])\n",
    "Y = accidents_sub.loc[:, ['Severity', 'Severity_new', 'Delay(min)', 'Delay_ln', 'Distance(km)']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4016917, 22), (4016917, 5), (912780, 22), (912780,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fxing imbalance by under sampling (random under sampling)\n",
    "rus = RandomUnderSampler(random_state=42, replacement=False)\n",
    "\n",
    "# fit predictor and target varialbe\n",
    "X_rus, Y_rus = rus.fit_resample(X, Y[\"Severity_new\"])\n",
    "\n",
    "X.shape, Y.shape, X_rus.shape, Y_rus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fxing imbalance by under sampling (Tomek links)\n",
    "tl = TomekLinks(sampling_strategy='majority', n_jobs=-1)\n",
    "\n",
    "# fit predictor and target variable\n",
    "X_tl, Y_tl = tl.fit_resample(X, Y[\"Severity_new\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Severity_new\n",
       "0    456390\n",
       "1    456390\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_rus.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4016917, 22), (4016917, 5), (821502, 22), (91278, 22), (821502,), (91278,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_rus, Y_rus, test_size = 0.10, random_state=370)\n",
    "\n",
    "assert len(X_train)  == len(Y_train)\n",
    "assert len(X_test)   == len(Y_test)\n",
    "X.shape, Y.shape,X_train.shape, X_test.shape, Y_train.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried to fit a regression model with Delay and distance as labels but no appropriate model have found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''model = SGDRegressor(max_iter=1000, tol=1e-3, random_state=370)\n",
    "\n",
    "model.fit(X_train, Y_train[\"Distance(km)\"])\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_train_pred = model.predict(X_train)\n",
    "\n",
    "# Calculate metrics\n",
    "mse = mean_squared_error(Y_train[\"Distance(km)\"], y_train_pred)\n",
    "r2 = r2_score(Y_train[\"Distance(km)\"], y_train_pred)\n",
    "print(f'Mean Squared Error on Test Set: {mse}')\n",
    "print(f'R_squared on Test Set: {r2}')'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(random_state=370, n_jobs=-1)\n",
    "rf = RandomForestClassifier(random_state=370, n_jobs=-1)\n",
    "sgb = xgboost.XGBClassifier(max_depth=1, n_estimator=100, n_jobs=-1)\n",
    "ada = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=100, learning_rate=0.001, random_state=370)\n",
    "\n",
    "models = [ lr, rf, sgb, ada]\n",
    "\n",
    "#perform cross validation using KFold\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "kfold = KFold(n_splits = 5, shuffle = True, random_state=370)\n",
    "\n",
    "for model in models:\n",
    "    score = cross_val_score(model, X_train, Y_train, cv=kfold, scoring='accuracy', n_jobs=-1)  #f1, recall, precision, accuracy\n",
    "    np.set_printoptions(precision = 2)\n",
    "    print(\"Model: \", model, \"Scores: \", score, \"- Scores mean: \", score.mean(), \"- Scores std (lower better):\", score.std()) \n",
    "    fit = model.fit(X_train, Y_train)\n",
    "    yhat = fit.predict(X_test)\n",
    "    print(confusion_matrix(Y_test, yhat))\n",
    "    print('ROCAUC score:',roc_auc_score(Y_test, yhat))      \n",
    "    print(classification_report(Y_test, yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_model = ada.fit(X_train, Y_train[\"Severity_new\"])\n",
    "yhat = ada_model.predict(X_test)\n",
    "print(classification_report(Y_test[\"Severity_new\"], yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\"n_estimators\": [10], \n",
    "              \"criterion\": [\"gini\", \"entropy\"],\n",
    "              \"max_depth\": 10}\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, Y_train[\"Severity\"])\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "print(classification_report(Y_test[\"Severity\"], yhat))'''\n",
    "\n",
    "'''grid = GridSearchCV(model, param_grid, refit=True)\n",
    "grid.fit(X_train, Y_train[\"Severity\"])\n",
    "\n",
    "print(grid.best_params_)\n",
    "\n",
    "yhat = grid.predict(X_test[\"Severity\"])\n",
    "\n",
    "print(classification_report(Y_test[\"Severity\"], yhat))'''\n",
    "'''from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import  classification_report\n",
    "\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1), n_estimators=200,\n",
    "    learning_rate=0.5, random_state=42)\n",
    "ada_clf.fit(X_train, Y_train[\"Severity\"])\n",
    "y_pred = ada_clf.predict(X_test)\n",
    "print(\"Ada score: \", accuracy_score(Y_test[\"Severity\"], y_pred))\n",
    "print(classification_report(Y_test[\"Severity\"], y_pred))'''\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "#not improved after 2 iterations\n",
    "xgb_reg.fit(X_train, Y_train[\"Distance(km)\"],\n",
    "                eval_set=[(X_test, Y_test[\"Distance(km)\"])])\n",
    "y_pred = xgb_reg.predict(X_test)\n",
    "mse = mean_squared_error(Y_test[\"Distance(km)\"], y_pred)\n",
    "r2 = r2_score(Y_test[\"Distance(km)\"], y_pred)\n",
    "\n",
    "print(\"MSE:\", mse)  #notice we are using mse while xgb uses root mse\n",
    "print(\"R2:\", r2)'''\n",
    "'''\n",
    "\n",
    "import torch\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "inputs = torch.from_numpy(X_train.values)\n",
    "target = torch.from_numpy(Y_train[\"Delay(min)\"].values)\n",
    "print(inputs.size())\n",
    "print(target.size())\n",
    "from torch.utils.data import TensorDataset\n",
    "# Define dataset\n",
    "train_ds = TensorDataset(inputs, target)\n",
    "from torch.utils.data import DataLoader\n",
    "# Define data loader\n",
    "batch_size = 500\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "# Build the ANN model\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Input layer\n",
    "model.add(Dense(128, input_shape=(X_train.shape[1],), activation='LeakyReLU'))\n",
    "\n",
    "# Hidden layers\n",
    "model.add(Dense(64, activation='LeakyReLU'))\n",
    "model.add(Dense(48, activation='LeakyReLU'))\n",
    "model.add(Dense(24, activation='LeakyReLU'))\n",
    "model.add(Dense(12, activation='LeakyReLU'))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(1, activation='linear'))  # Linear activation for regression\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, Y_train[\"Distance(km)\"], epochs=1, batch_size=100, validation_data=(X_test, Y_test[\"Distance(km)\"]))\n",
    "yhat = model.predict(X_test)\n",
    "r2 = r2_score(Y_test[\"Distance(km)\"], yhat)\n",
    "weights_and_biases = model.get_weights()\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X_test, Y_test[\"Distance(km)\"])\n",
    "print(f'Mean Squared Error on Test Set: {loss:.2f}')\n",
    "print(r2)\n",
    "'''\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#sum of squared distances\n",
    "ssd = []\n",
    "for k in range(2, 20):\n",
    "    kmeans = KMeans(n_clusters=k, n_init='auto')\n",
    "    kmeans.fit(X)\n",
    "    ssd.append(kmeans.inertia_)\n",
    "\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "plt.plot(range(2, 20), ssd)\n",
    "plt.xticks(range(2, 20))\n",
    "plt.grid(True)\n",
    "plt.ylabel(\"within cluster variation\")\n",
    "plt.xlabel(\"k\")\n",
    "plt.title('Elbow curve')\n",
    "\n",
    "plt.annotate('elbow', xy=(4.3, 220), xytext=(5, 600),  #xytext ---> xy\n",
    "            arrowprops=dict(arrowstyle=\"->\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
